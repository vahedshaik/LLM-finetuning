{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "I have created a medical chatbot using the Mistral7, with RAG and Llama index. I have prepared a custom dataset using GPT-4 which specializes in mental health care conversations and books that talk about mental health care. The chatbot guides users, provides personalized and emphatetic responses, helping user with their mental health wellness"
      ],
      "metadata": {
        "id": "XupxL659Gs1v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install and import Required libraries"
      ],
      "metadata": {
        "id": "387oAcp0HYNn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9efInp3Wlup",
        "outputId": "e58f400b-9613-4d18-9a0b-7cf4e8f056cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/277.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m225.3/277.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m277.4/277.4 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q pypdf\n",
        "!pip install -q python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wFP3ngwWwLo",
        "outputId": "1a2506d2-6dc2-4d3c-8ecb-75bbdff4772b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make use of CPU and GPU for inferencing from my LLM"
      ],
      "metadata": {
        "id": "L3bItVH3D6Js"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install  llama-cpp-python --no-cache-dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWLstQLPCjk1",
        "outputId": "dac2b308-9123-492d-c981-0655968ff7a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.2.12.tar.gz (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.23.5)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (5.6.3)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.12-cp310-cp310-manylinux_2_35_x86_64.whl size=4032366 sha256=7aabdff195942c08826762d2eea8c7bd715968e7e810fafdf97c0731ef250a3a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-cd9kfnvu/wheels/07/07/dd/f9549e7045690b8b66dadaa2224a478aa49a61edc8d368a62e\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: llama-cpp-python\n",
            "Successfully installed llama-cpp-python-0.2.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Llama index library supports retrieval based augmentation for LLMs"
      ],
      "metadata": {
        "id": "QiEkElj7ECjd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can create embeddings for the text in PDF. So when I query something, I am able to get answer for my query from the mistral model"
      ],
      "metadata": {
        "id": "kwhMJ-VfENQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q llama-index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxTtFeeUW5WJ",
        "outputId": "28c17726-7faa-42ed-ba94-494cb2fe1cd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m808.0/808.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import sys\n",
        "\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
        "\n",
        "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n"
      ],
      "metadata": {
        "id": "lkHavErBXChp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = SimpleDirectoryReader(\"/content/Data/\").load_data()"
      ],
      "metadata": {
        "id": "COxp34FqXMih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from llama_index.llms import LlamaCPP\n",
        "from llama_index.llms.llama_utils import messages_to_prompt, completion_to_prompt\n",
        "llm = LlamaCPP(\n",
        "    # You can pass in the URL to a GGML model to download it automatically\n",
        "    model_url='https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf',\n",
        "    # optionally, you can set the path to a pre-downloaded model instead of model_url\n",
        "    model_path=None,\n",
        "    temperature=0.1,\n",
        "    max_new_tokens=256,\n",
        "    # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room\n",
        "    context_window=3900,\n",
        "    # kwargs to pass to __call__()\n",
        "    generate_kwargs={},\n",
        "    # kwargs to pass to __init__()\n",
        "    # set to at least 1 to use GPU\n",
        "    model_kwargs={\"n_gpu_layers\": -1},\n",
        "    # transform inputs into Llama2 format\n",
        "    messages_to_prompt=messages_to_prompt,\n",
        "    completion_to_prompt=completion_to_prompt,\n",
        "    verbose=True,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhyzesSOYuRf",
        "outputId": "ad162584-7804-4b58-81bb-605c06a7d055"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKfxkosuLxOW",
        "outputId": "d282a60d-8c7a-412b-a863-e9ac050cb332"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/86.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from llama_index import LangchainEmbedding, ServiceContext\n",
        "\n",
        "embed_model = LangchainEmbedding(\n",
        "  HuggingFaceEmbeddings(model_name=\"thenlper/gte-large\")\n",
        ")\n"
      ],
      "metadata": {
        "id": "6mT0mMqtctGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "service_context = ServiceContext.from_defaults(\n",
        "    chunk_size=256,\n",
        "    llm=llm,\n",
        "    embed_model=embed_model\n",
        ")"
      ],
      "metadata": {
        "id": "EqUPeKH6crpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = VectorStoreIndex.from_documents(documents, service_context=service_context)"
      ],
      "metadata": {
        "id": "bwGjWnoNai9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"I feel the urge to overeat everyday, what to do?\")"
      ],
      "metadata": {
        "id": "P738v53ganSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1-T4aVNdSn9",
        "outputId": "a1d41ef7-bfa2-4e5c-f937-c4321733db2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " It's understandable that you may feel the urge to overeat every day. One approach could be to identify any patterns in your eating habits, such as specific times when you eat more or less, or certain emotions that trigger changes in your appetite. Addressing the root cause of the stress and finding healthier coping mechanisms can contribute to stabilizing your eating habits. Additionally, starting with small, manageable steps, such as adding one serving of vegetables to one meal a day or choosing a piece of fruit as a snack, can make the process more achievable. Planning meals in advance and having healthy options readily available can also help reduce overwhelm and make healthier choices easier. It's important to remember that making changes can feel overwhelming, but taking small steps consistently can lead to long-term improvements in your nutritional intake and overall well-being.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inorder to end the conversation I clicked on stop running. This is not an error"
      ],
      "metadata": {
        "id": "WHQn9cHyJjoG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "  query=input()\n",
        "  response = query_engine.query(query)\n",
        "  print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 845
        },
        "id": "Gh3JY0D6hAiB",
        "outputId": "64d9568b-5811-4ea8-c623-feacdcd3972f"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I have been having difficulties with sleeping every night. Could you help me?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Yes, I can definitely help you with your sleep difficulties. It sounds like you may be experiencing some common issues that can impact sleep quality, such as stress and difficulty relaxing before bed. One helpful strategy to try is creating a calming bedtime routine that includes activities like reading, gentle stretching, or listening to calming music. Additionally, limiting screen time before bed can also help improve sleep quality. Have you tried any relaxation techniques before bed, such as deep breathing exercises? If not, it might be worth giving them a try. Remember, creating a consistent sleep routine can contribute to better sleep over time.\n",
            "I try to go to bed at the same time everyday, but inspite of trying hard to close my eyes and lay down, I cant sleep. I keep getting some thoughts\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " It sounds like you are having difficulty falling asleep despite trying to establish a consistent sleep routine. There could be several reasons for this, such as stress or anxiety, physical discomfort, or an irregular sleep schedule. One strategy that may help is to engage in relaxation techniques before bedtime, such as deep breathing exercises, progressive muscle relaxation, or meditation. It's also important to create a comfortable sleep environment by keeping your bedroom cool, dark, and quiet, and investing in a supportive mattress and pillows. If you continue to have difficulty sleeping despite making these changes, it may be helpful to consult with a healthcare professional who can provide further guidance and support.\n",
            "Is there a specific phone number or website that I can check inorder to check my issue\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Based on the given context information, if you are experiencing issues related to mental health, such as peer pressure, substance use, or gender identity, there are several resources available to you. For example, if you're feeling pressured to engage in risky behaviors, it's important to talk to a trusted adult. The Substance Abuse and Mental Health Services Administration (SAMHSA) also has a national helpline at 1-800-662-HELP (4357) where you can get confidential information and support. If you are experiencing issues related to gender identity, the Trevor Project provides support for LGBTQ+ youth, including a 24/7 helpline at 1-866-488-7386. GLAAD also has resources for understanding and navigating gender identity. Additionally, you can find local therapists and counselors through Psychology Today's Therapist Directory. If you need immediate assistance, you can also use the Crisis Text Line by texting \"HELLO\" to 741741 for supportive text conversation.\n",
            "Can you provide me a list of activities I can try to get sleep on time\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Based on the context information provided, here are some activities that you can try to improve your sleep routine and get a good night's sleep:\n",
            "\n",
            "1. Reading a book or an article\n",
            "2. Gentle stretching or yoga\n",
            "3. Listening to calming music\n",
            "4. Limiting screen time before bed\n",
            "5. Setting boundaries around work hours to ensure you have time to unwind before bed\n",
            "\n",
            "It's important to note that everyone is different, so it may take some trial and error to find the activities that work best for you. Additionally, creating a consistent sleep schedule and sticking to it can also help improve your sleep quality over time.\n",
            "thanks for your help\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " You're welcome! Remember that taking care of your mental health is a vital part of your overall well-being. If you need any further support or guidance in the future, don't hesitate to reach out.\n",
            "exit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " I understand that you would like to exit this conversation. If you or someone you know is experiencing thoughts of suicide, it is crucial to contact emergency services, a crisis hotline, or a trusted individual who can assist in connecting with the appropriate support. Your safety is the top priority. Is there anything else I can help you with?\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-e57627802c61>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jbikmS8ulJ91"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}